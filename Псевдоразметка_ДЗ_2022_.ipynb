{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1-8AS8I5x-H"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHDmcbfN5rgg"
   },
   "source": [
    "**Использование псевдоразметки. ДЗ.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sxnb33uad1d5"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A1YpI_9a5rgo"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auP4TzJO5rgo"
   },
   "source": [
    "Начнем с загрузки датасета. Речевые данные (и модели, обучаемые на них) очень тяжелые, поэтому мы обойдемся чем-нибудь попроще."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kontsevaya/speech'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wKvD1Q8gdWkL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88566d5ae4794949983d786d175ec6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69829949929344b4ac137d24e8973aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e19ec0ebd7d4d5f9a8041814b9ed05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4251b3440a324b41b09d9b155f7c9919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kontsevaya/sberocrs/sber-ocr-2/.sberocr-env2/lib/python3.6/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = \\\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "test_dataset = \\\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9lXwKaeg5rgp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKuoY7I05rgq"
   },
   "source": [
    "Итак, трейн состоит из 60000 картинок цифр. Для того, чтобы получше увидеть эффект от псевдолейблов, мы оставим только 100 этих картинок в качестве размеченных данных. Остальные 59900 будут в качестве неразмеченных. \n",
    "\n",
    "На масштабах 100 записей могут проявиться неприятные эффекты, если какие-то из классов не будут достаточно хорошо представлены. Чтобы этого избежать, будем аккуратно семплировать. Самый простой вариант - просто случайно разделять, пока не получится удачное разбиение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWeomwqW5rgq"
   },
   "source": [
    "Для начала определим удачность разбиения. Будем считать размеченный датасет хорошим, если из 100 примеров в нем есть хотя бы по 8 представителей каждого класса. Напишите функцию, которая делает такую проверку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "45G7LlRf5rgq"
   },
   "outputs": [],
   "source": [
    "AT_LEAST = 8\n",
    "\n",
    "def check_dataset(dataset):\n",
    "    label_dict = {i: 0 for i in range(10)}\n",
    "    for item in dataset:\n",
    "        label = item[1]\n",
    "        label_dict[label] +=1\n",
    "        if all(v >= AT_LEAST for v in label_dict.values()):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "scjxRQfR5rgr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "Split the dataset after 12 resamplings\n"
     ]
    }
   ],
   "source": [
    "sampling_iteration = 0\n",
    "while True:\n",
    "    labeled_train_dataset, unlabeled_train_dataset = torch.utils.data.random_split(train_dataset, [100, 59900])\n",
    "    if check_dataset(labeled_train_dataset):\n",
    "        break\n",
    "    sampling_iteration += 1\n",
    "    print(sampling_iteration)\n",
    "print(f'Split the dataset after {sampling_iteration} resamplings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "9XDTdYXunhve"
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=False)\n",
    "labeled_train_loader = torch.utils.data.DataLoader(\n",
    "    labeled_train_dataset, batch_size=64, shuffle=True)\n",
    "unlabeled_train_loader = torch.utils.data.DataLoader(\n",
    "    unlabeled_train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xyb9Uk505rgs"
   },
   "source": [
    "Теперь, когда мы получили данные, определим архитектуру сети. Возьмем простую сверточную сетку с droupout'ом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "BXHh9QBMi1md"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(20, 40, kernel_size=5)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "        self.fc1 = nn.Linear(640, 150)\n",
    "        self.fc2 = nn.Linear(150, 10)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.dropout(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 640)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaG1bxhI5rgt"
   },
   "source": [
    "Опишем вспомогательные функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "UwbKihvcjsIl"
   },
   "outputs": [],
   "source": [
    "def train(epoch_idx, model, optimizer, train_loader, loss_func=F.nll_loss):\n",
    "    model.train()\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        x, target = x.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "esbjg0S0lF-a"
   },
   "outputs": [],
   "source": [
    "def test(epoch_idx, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, target in test_loader:\n",
    "            x, target = x.cuda(), target.cuda()\n",
    "            output = model(x)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Epoch {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        epoch_idx, test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "WzVXnGtKc_pf"
   },
   "outputs": [],
   "source": [
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            result.append(model(x.cuda()))\n",
    "    return torch.cat(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ga7bO-Sx5rgu"
   },
   "source": [
    "Создадим модель и обучим ее на нашем размеченном датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "X831vbLVuAjY"
   },
   "outputs": [],
   "source": [
    "model = Net().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "h_jXa_zUuGbi",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kontsevaya/sberocrs/sber-ocr-2/.sberocr-env2/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/kontsevaya/sberocrs/sber-ocr-2/.sberocr-env2/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Average loss: 2.2824, Accuracy: 1829/10000 (18%)\n",
      "Epoch 10: Average loss: 1.8417, Accuracy: 4466/10000 (45%)\n",
      "Epoch 20: Average loss: 0.9893, Accuracy: 6422/10000 (64%)\n",
      "Epoch 30: Average loss: 0.6253, Accuracy: 7761/10000 (78%)\n",
      "Epoch 40: Average loss: 0.6356, Accuracy: 8159/10000 (82%)\n",
      "Epoch 50: Average loss: 0.7345, Accuracy: 7889/10000 (79%)\n",
      "Epoch 60: Average loss: 0.6410, Accuracy: 8368/10000 (84%)\n",
      "Epoch 70: Average loss: 0.6432, Accuracy: 8450/10000 (84%)\n",
      "Epoch 80: Average loss: 0.6390, Accuracy: 8456/10000 (85%)\n",
      "Epoch 90: Average loss: 0.6925, Accuracy: 8425/10000 (84%)\n",
      "Epoch 100: Average loss: 0.6618, Accuracy: 8478/10000 (85%)\n",
      "Epoch 110: Average loss: 0.7043, Accuracy: 8455/10000 (85%)\n",
      "Epoch 120: Average loss: 0.8320, Accuracy: 8346/10000 (83%)\n",
      "Epoch 130: Average loss: 0.7793, Accuracy: 8409/10000 (84%)\n",
      "Epoch 140: Average loss: 0.7055, Accuracy: 8547/10000 (85%)\n",
      "Epoch 150: Average loss: 0.7872, Accuracy: 8444/10000 (84%)\n",
      "Epoch 160: Average loss: 0.8246, Accuracy: 8453/10000 (85%)\n",
      "Epoch 170: Average loss: 0.7295, Accuracy: 8534/10000 (85%)\n",
      "Epoch 180: Average loss: 0.8674, Accuracy: 8412/10000 (84%)\n",
      "Epoch 190: Average loss: 0.7785, Accuracy: 8513/10000 (85%)\n",
      "Epoch 200: Average loss: 0.8450, Accuracy: 8467/10000 (85%)\n",
      "Epoch 210: Average loss: 0.8229, Accuracy: 8465/10000 (85%)\n",
      "Epoch 220: Average loss: 0.7629, Accuracy: 8560/10000 (86%)\n",
      "Epoch 230: Average loss: 0.8227, Accuracy: 8547/10000 (85%)\n",
      "Epoch 240: Average loss: 0.8008, Accuracy: 8572/10000 (86%)\n",
      "Epoch 250: Average loss: 0.8606, Accuracy: 8537/10000 (85%)\n",
      "Epoch 260: Average loss: 0.8973, Accuracy: 8551/10000 (86%)\n",
      "Epoch 270: Average loss: 0.8969, Accuracy: 8534/10000 (85%)\n",
      "Epoch 280: Average loss: 0.8489, Accuracy: 8509/10000 (85%)\n",
      "Epoch 290: Average loss: 0.9009, Accuracy: 8434/10000 (84%)\n",
      "Epoch 300: Average loss: 0.8561, Accuracy: 8484/10000 (85%)\n",
      "Epoch 310: Average loss: 0.9499, Accuracy: 8462/10000 (85%)\n",
      "Epoch 320: Average loss: 0.9028, Accuracy: 8511/10000 (85%)\n",
      "Epoch 330: Average loss: 0.8944, Accuracy: 8521/10000 (85%)\n",
      "Epoch 340: Average loss: 0.8890, Accuracy: 8522/10000 (85%)\n",
      "Epoch 350: Average loss: 0.8539, Accuracy: 8572/10000 (86%)\n",
      "Epoch 360: Average loss: 0.9076, Accuracy: 8517/10000 (85%)\n",
      "Epoch 370: Average loss: 0.9306, Accuracy: 8512/10000 (85%)\n",
      "Epoch 380: Average loss: 0.9593, Accuracy: 8506/10000 (85%)\n",
      "Epoch 390: Average loss: 0.9825, Accuracy: 8505/10000 (85%)\n"
     ]
    }
   ],
   "source": [
    "for i in range(400):\n",
    "    train(i, model, optimizer, labeled_train_loader)\n",
    "    if i % 10 == 0:\n",
    "        test(i, model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([59900, 10])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, list(unlabeled_train_dataset)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRFYDeIg5rgv"
   },
   "source": [
    "Теперь попробуем побить этот результат с помощью псевдолейблов. Напишем функцию, которая принимает модель и возращает DataLoader с хард-лейблами, и запустим обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "f1FwrOLe5rgw"
   },
   "outputs": [],
   "source": [
    "def get_pseudo_loader(model):\n",
    "    dataset = list(unlabeled_train_dataset)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(dataset):\n",
    "            pred = model(x.cuda())\n",
    "            dataset[i] = (x, np.argmax(pred.cpu()))\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "lbFpYgqC5rgw"
   },
   "outputs": [],
   "source": [
    "model_hard = Net().cuda()\n",
    "model_hard.load_state_dict(model.state_dict())\n",
    "optimizer_hard = torch.optim.SGD(model_hard.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "i4WjNPrM2qZi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kontsevaya/sberocrs/sber-ocr-2/.sberocr-env2/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Average loss: 0.5427, Accuracy: 8667/10000 (87%)\n",
      "Epoch 1: Average loss: 0.5580, Accuracy: 8707/10000 (87%)\n",
      "Epoch 2: Average loss: 0.5911, Accuracy: 8694/10000 (87%)\n",
      "Epoch 3: Average loss: 0.5596, Accuracy: 8697/10000 (87%)\n",
      "Epoch 4: Average loss: 0.6305, Accuracy: 8710/10000 (87%)\n",
      "Epoch 5: Average loss: 0.7311, Accuracy: 8625/10000 (86%)\n",
      "Epoch 6: Average loss: 0.6505, Accuracy: 8644/10000 (86%)\n",
      "Epoch 7: Average loss: 0.6118, Accuracy: 8696/10000 (87%)\n",
      "Epoch 8: Average loss: 0.6262, Accuracy: 8633/10000 (86%)\n",
      "Epoch 9: Average loss: 0.5810, Accuracy: 8704/10000 (87%)\n"
     ]
    }
   ],
   "source": [
    "hard_labeled_loader = get_pseudo_loader(model)\n",
    "for i in range(10):\n",
    "    train(i, model_hard, optimizer_hard, hard_labeled_loader)\n",
    "    train(i, model_hard, optimizer_hard, labeled_train_loader)\n",
    "    test(i, model_hard, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gcvAXOY5rgx"
   },
   "source": [
    "**Итеративная псевдоразметка.**\n",
    "\n",
    "Мы уже видим небольшое улучшение, но можно пойти дальше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "oZ00dzMK5rgx"
   },
   "outputs": [],
   "source": [
    "model_hard_iter = Net().cuda()\n",
    "model_hard_iter.load_state_dict(model.state_dict())\n",
    "optimizer_hard_iter = torch.optim.SGD(model_hard_iter.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "REl_v-Sf5rgx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Average loss: 0.5711, Accuracy: 8702/10000 (87%)\n",
      "Epoch 1: Average loss: 0.6094, Accuracy: 8830/10000 (88%)\n",
      "Epoch 2: Average loss: 0.5249, Accuracy: 8918/10000 (89%)\n",
      "Epoch 3: Average loss: 0.6092, Accuracy: 8883/10000 (89%)\n",
      "Epoch 4: Average loss: 0.6089, Accuracy: 8943/10000 (89%)\n",
      "Epoch 5: Average loss: 0.5638, Accuracy: 8986/10000 (90%)\n",
      "Epoch 6: Average loss: 0.6328, Accuracy: 8969/10000 (90%)\n",
      "Epoch 7: Average loss: 0.5571, Accuracy: 9037/10000 (90%)\n",
      "Epoch 8: Average loss: 0.5933, Accuracy: 9035/10000 (90%)\n",
      "Epoch 9: Average loss: 0.6800, Accuracy: 9017/10000 (90%)\n",
      "Epoch 10: Average loss: 0.6030, Accuracy: 9060/10000 (91%)\n",
      "Epoch 11: Average loss: 0.6247, Accuracy: 9055/10000 (91%)\n",
      "Epoch 12: Average loss: 0.6847, Accuracy: 9061/10000 (91%)\n",
      "Epoch 13: Average loss: 0.7165, Accuracy: 9058/10000 (91%)\n",
      "Epoch 14: Average loss: 0.6640, Accuracy: 9060/10000 (91%)\n",
      "Epoch 15: Average loss: 0.7630, Accuracy: 9056/10000 (91%)\n",
      "Epoch 16: Average loss: 0.6991, Accuracy: 9073/10000 (91%)\n",
      "Epoch 17: Average loss: 0.7718, Accuracy: 9067/10000 (91%)\n",
      "Epoch 18: Average loss: 0.7640, Accuracy: 9061/10000 (91%)\n",
      "Epoch 19: Average loss: 0.7441, Accuracy: 9087/10000 (91%)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    hard_labeled_loader = get_pseudo_loader(model_hard_iter)\n",
    "    train(i, model_hard_iter, optimizer_hard_iter, hard_labeled_loader)\n",
    "    train(i, model_hard_iter, optimizer_hard_iter, labeled_train_loader)\n",
    "    test(i, model_hard_iter, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri3o1-Mz5rgy"
   },
   "source": [
    "**Оценивание.**\n",
    "\n",
    "В предыдущем пункте нужно получить accuracy 91% или выше (5 баллов).\n",
    "\n",
    "Следующие шаги:\n",
    "\n",
    "Модифицировать функцию `get_pseudo_loader`, чтобы она могла возвращать софт-лейблы (+1 балл).\n",
    "\n",
    "Правильно запустить обучение - в качестве лосса используем KL-дивергенцию. Получить accuracy 90% или выше. (+3 балла).\n",
    "\n",
    "Интуитивно кажется, что модель не должна ничему учиться, т.к. ее выход будет полностью совпадать с софт-лейблами. Напишите (текстом), почему тем не менее удается сильно выиграть относительно бейзлайна. (+1 балл)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "BXezhZ325rgy"
   },
   "outputs": [],
   "source": [
    "model_soft_iter = Net().cuda()\n",
    "model_soft_iter.load_state_dict(model.state_dict())\n",
    "optimizer_soft_iter = torch.optim.SGD(model_soft_iter.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "UA5DmEKV5rgz"
   },
   "outputs": [],
   "source": [
    "def get_pseudo_loader(model, soft=False):\n",
    "    dataset = list(unlabeled_train_dataset)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(dataset):\n",
    "            pred = model(x.cuda())\n",
    "            if soft:\n",
    "                dataset[i] = (x, pred.cpu().squeeze())\n",
    "            else:\n",
    "                dataset[i] = (x, np.argmax(pred.cpu()))\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "KD-uX-Cl5rgz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Average loss: 0.8990, Accuracy: 8538/10000 (85%)\n",
      "Epoch 1: Average loss: 0.9005, Accuracy: 8479/10000 (85%)\n",
      "Epoch 2: Average loss: 0.8583, Accuracy: 8558/10000 (86%)\n",
      "Epoch 3: Average loss: 0.8596, Accuracy: 8556/10000 (86%)\n",
      "Epoch 4: Average loss: 0.8613, Accuracy: 8561/10000 (86%)\n",
      "Epoch 5: Average loss: 0.8625, Accuracy: 8554/10000 (86%)\n",
      "Epoch 6: Average loss: 0.8713, Accuracy: 8553/10000 (86%)\n",
      "Epoch 7: Average loss: 0.8674, Accuracy: 8562/10000 (86%)\n",
      "Epoch 8: Average loss: 0.8914, Accuracy: 8560/10000 (86%)\n",
      "Epoch 9: Average loss: 0.8268, Accuracy: 8550/10000 (86%)\n",
      "Epoch 10: Average loss: 0.8276, Accuracy: 8550/10000 (86%)\n",
      "Epoch 11: Average loss: 0.8300, Accuracy: 8550/10000 (86%)\n",
      "Epoch 12: Average loss: 0.8299, Accuracy: 8556/10000 (86%)\n",
      "Epoch 13: Average loss: 0.8937, Accuracy: 8482/10000 (85%)\n",
      "Epoch 14: Average loss: 0.8544, Accuracy: 8526/10000 (85%)\n",
      "Epoch 15: Average loss: 0.8503, Accuracy: 8539/10000 (85%)\n",
      "Epoch 16: Average loss: 0.8524, Accuracy: 8538/10000 (85%)\n",
      "Epoch 17: Average loss: 0.8592, Accuracy: 8536/10000 (85%)\n",
      "Epoch 18: Average loss: 0.9407, Accuracy: 8437/10000 (84%)\n",
      "Epoch 19: Average loss: 0.9732, Accuracy: 8424/10000 (84%)\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "for i in range(20):\n",
    "    soft_labeled_loader = get_pseudo_loader(model_soft_iter, soft=True)\n",
    "    train(i, model_soft_iter, optimizer_soft_iter, soft_labeled_loader, loss_func=criterion)\n",
    "    train(i, model_soft_iter, optimizer_soft_iter, labeled_train_loader)\n",
    "    test(i, model_soft_iter, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Average loss: 0.9000, Accuracy: 8545/10000 (85%)\n",
      "Epoch 1: Average loss: 0.8910, Accuracy: 8566/10000 (86%)\n",
      "Epoch 2: Average loss: 0.8929, Accuracy: 8569/10000 (86%)\n",
      "Epoch 3: Average loss: 0.9373, Accuracy: 8445/10000 (84%)\n",
      "Epoch 4: Average loss: 0.9397, Accuracy: 8447/10000 (84%)\n",
      "Epoch 5: Average loss: 0.9428, Accuracy: 8450/10000 (84%)\n",
      "Epoch 6: Average loss: 0.9515, Accuracy: 8454/10000 (85%)\n",
      "Epoch 7: Average loss: 0.9446, Accuracy: 8459/10000 (85%)\n",
      "Epoch 8: Average loss: 0.9502, Accuracy: 8461/10000 (85%)\n",
      "Epoch 9: Average loss: 0.9476, Accuracy: 8478/10000 (85%)\n",
      "Epoch 10: Average loss: 0.9488, Accuracy: 8479/10000 (85%)\n",
      "Epoch 11: Average loss: 0.9482, Accuracy: 8479/10000 (85%)\n",
      "Epoch 12: Average loss: 0.9474, Accuracy: 8480/10000 (85%)\n",
      "Epoch 13: Average loss: 0.9432, Accuracy: 8490/10000 (85%)\n",
      "Epoch 14: Average loss: 0.9431, Accuracy: 8491/10000 (85%)\n",
      "Epoch 15: Average loss: 0.9479, Accuracy: 8491/10000 (85%)\n",
      "Epoch 16: Average loss: 0.9482, Accuracy: 8492/10000 (85%)\n",
      "Epoch 17: Average loss: 0.9410, Accuracy: 8499/10000 (85%)\n",
      "Epoch 18: Average loss: 0.9364, Accuracy: 8507/10000 (85%)\n",
      "Epoch 19: Average loss: 0.9456, Accuracy: 8501/10000 (85%)\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.KLDivLoss(reduction='sum')\n",
    "\n",
    "for i in range(20):\n",
    "    soft_labeled_loader = get_pseudo_loader(model_soft_iter, soft=True)\n",
    "    train(i, model_soft_iter, optimizer_soft_iter, soft_labeled_loader, loss_func=criterion)\n",
    "    train(i, model_soft_iter, optimizer_soft_iter, labeled_train_loader)\n",
    "    test(i, model_soft_iter, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
